import torch
import torch.nn as nn
import torch.nn.functional as F

BATCH_SIZE = 64
EMBED_SIZE = 300
NUM_FEATURE_MAPS = 300 # feature maps generated by each kernel
KERNEL_SIZES = [2, 3, 4, 5]
DROPOUT = 0.5
LEARNING_RATE = 0.01
WEIGHT_DECAY = 1e-4
VERBOSE = False
SAVE_EVERY = 10

PAD = "<PAD>" # padding
PAD_IDX = 0

torch.manual_seed(1)
CUDA = torch.cuda.is_available()

class word_cnn(nn.Module):
    def __init__(self, vocab_size, num_labels):
        super().__init__()

        # architecture
        self.embed = nn.Embedding(vocab_size, EMBED_SIZE, padding_idx = PAD_IDX)
        self.conv = nn.ModuleList([nn.Conv2d(1, NUM_FEATURE_MAPS, (i, EMBED_SIZE)) for i in KERNEL_SIZES])
        self.dropout = nn.Dropout(DROPOUT)
        self.fc = nn.Linear(len(KERNEL_SIZES) * NUM_FEATURE_MAPS, num_labels)
        self.softmax = nn.LogSoftmax(1)

        if CUDA:
            self = self.cuda()

    def forward(self, x):
        x = self.embed(x) # [batch_size (N), seq_len (H), embed_size (W)]
        x = x.unsqueeze(1) # [N, in_channels (Ci), H, W]
        h = [conv(x) for conv in self.conv] # [N, out_channels (Co), H, W] * num_kernels (K)
        h = [F.relu(k).squeeze(3) for k in h] # [N, Co, H] * K
        h = [F.max_pool1d(k, k.size(2)).squeeze(2) for k in h] # [N, Co] * K
        h = torch.cat(h, 1) # [N, Co * K]
        h = self.dropout(h)
        h = self.fc(h) # fully connected layer 
        y = self.softmax(h)
        return y

def LongTensor(*args):
    x = torch.LongTensor(*args)
    return x.cuda() if CUDA else x

def scalar(x):
    return x.view(-1).data.tolist()[0]

def argmax(x):
    return scalar(torch.max(x, 0)[1]) # for 1D tensor
