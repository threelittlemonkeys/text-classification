UNIT = "word" # unit of tokenization (char, word)
BATCH_SIZE = 64
EMBED = ["char", "word"] # embeddings (char, word)
EMBED_SIZE = 300
NUM_FEATURE_MAPS = 100 # feature maps generated by each kernel
KERNEL_SIZES = [2, 3, 4, 5]
DROPOUT = 0.5
LEARNING_RATE = 1e-4
VERBOSE = False
EVAL_EVERY = 10
SAVE_EVERY = 10

PAD = "<PAD>" # padding
SOS = "<SOS>" # start of sequence
EOS = "<EOS>" # end of sequence
UNK = "<UNK>" # unknown token

PAD_IDX = 0
SOS_IDX = 1
EOS_IDX = 2
UNK_IDX = 3

NUM_DIGITS = 4 # number of digits to print
